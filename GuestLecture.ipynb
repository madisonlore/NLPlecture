{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd90f55",
   "metadata": {},
   "source": [
    "## NLP Lecture for Advanced Urban Analytics\n",
    "\n",
    "In this lecture, you will be introduced to using Natural Language Processing (NLP) in urban analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e3b10",
   "metadata": {},
   "source": [
    "Objectives for this lecture:\n",
    "\n",
    "1. Understand and use common NLP python packages.\n",
    "2. Find and visualize patterns in language topics. \n",
    "3. Relate language and topics to the underlying urban landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677d9ce",
   "metadata": {},
   "source": [
    "### What is NLP and how can you use it?\n",
    "\n",
    "NLP is ability to process text or spoken word based data with a computer in order to efficiently deal with large, potentially unruly or unstructured, data. \n",
    "\n",
    "In urban analytics, the uses of NLP are boundless! You can now handle large amounts of data coming from plans themselves, online open response questionaires, social media postings, transcripts from interviews or meetings, and more. Each of these datasets can illuminate important themes that may be difficult or time consuming to find by hand.\n",
    "\n",
    "The NLP processing chain is most often:\n",
    "1. Preprocess data to make text as uniform as possible.\n",
    "2. Decide what each \"document\" should be - whole body, paragraph, sentence, few words, etc.\n",
    "3. Turn each document into vector.\n",
    "4. Utilize various existing tools with vectorized data.\n",
    "5. Analyze results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import transformers\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa07527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work with some example data from Zillow \n",
    "data = pd.read_csv('/Users/madilore/Desktop/newyork_housing.csv')\n",
    "data['description'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67317dc7-4c24-4ba5-b13a-005d4dddae28",
   "metadata": {},
   "source": [
    "### 1. Preprocessing data\n",
    "\n",
    "Ultimately, you are working with one string composed of different symbols (letters and numbers), so creating uniformity however possible is helpful. You want the computer to recognize \"T\" and \"t\" as the same symbol. You might not also want your application to care about \"*\" or \"|\". It all depends on what you want to pick up on.\n",
    "\n",
    "#### 1.1 Regex Commands\n",
    "\n",
    "##### - forcing lowercase and removing unwanted symbols\n",
    "\n",
    "A great tool for manipulating string/text based data is regex. \"regex\" is shorthand for \"regular expression\" and is a programming language in itself. There are forms of composing a regex command in order to change a string into something else. \n",
    "\n",
    "Some important regex commands:\n",
    "1. ```[ ]``` tells it to look for character types that fall in between the brackets\n",
    "2. ```^``` tells it to perform an action on anything EXCEPT the characters that follow ^\n",
    "3. ```*``` tells it to look for a character zero or more times. Ex. ```ca*t``` will match ```ct, cat, caat,``` etc.\n",
    "4. ```+``` is similar to ```*```, but it looks for a match of one or more times. Ex. ```ca+t``` will only match ```cat, caat,``` etc.\n",
    "5. ```?``` tells it that a character is optional, or that it can be there zero or one times. Ex. ```ca?t``` will match ```ct, cat```\n",
    "6. ```a-c``` tells it to look for all characters that fall between the two given, with numerical or alphabetical order. Ex. ```a-z``` will look at the full alphabet, ```0-9```will look at all digits.\n",
    "\n",
    "Using regex: \n",
    "1. ```re.compile``` This can be used to save out a regex command. Think of it kind of like a function. Ex. ```t = re.compile('[0-9]+')``` saves the command \"look for all digits if they appear at least once\" as variable \"t\"\n",
    "2. ```re.findall``` Now, we can use our compiled command against sets of text data to find all instances that match the command. Ex. ```t.findall('I have 10 oranges, 8 apples, and 6 pears')``` will return ```['10','8','6']```\n",
    "3. ```re.sub``` We can also change what the text says with regex. Ex. ```t.sub('number', 'I have 10 oranges, 8 apples, and 6 pears')``` will return ```'I have number oranges, number apples, and number pears'```. You can also call re.sub directly with ```re.sub(expression, replacement, string)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate the text column\n",
    "bodytext = data['description']\n",
    "#make all letters lowercase\n",
    "bodytext = bodytext.str.lower()\n",
    "#remove non alphabetic characters \n",
    "bodytext = bodytext.apply(lambda x: re.sub(\"[^A-Za-z']+\", ' ', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the before\n",
    "print(data['description'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03590280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the after\n",
    "print(bodytext.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9aa632",
   "metadata": {},
   "source": [
    "#### 1.2 Introduction to nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b2b60",
   "metadata": {},
   "source": [
    "One of the most powerful tools in Python for NLP is the natural langauge toolkit (nltk) (https://www.nltk.org/). It is rich with processes and easy to use. Often, this package is used for the preprocessing stage where your text data may undergo any of the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a65f1",
   "metadata": {},
   "source": [
    "##### - removing \"stopwords\"\n",
    "\n",
    "Stopwords are very common, usually insignificant words that you want filtered out before you do any processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6602343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at some of the \"stopwords\"\n",
    "nltk.corpus.stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8970b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove these from each document\n",
    "bodytext = bodytext.apply(lambda x: x.split(\" \"))\n",
    "no_stopwords = bodytext.apply(lambda x: sorted(set(x) - set(nltk.corpus.stopwords.words('english')), key=x.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40264876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now view our sample text without any stopwords \n",
    "print(no_stopwords.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a35ad",
   "metadata": {},
   "source": [
    "#### - stemming or lemmatizing \n",
    "\n",
    "Stemming is the process of taking a word down to its root. Lemmatizing is the process of changing a word to its base format. Either step is usually performed in order to help your model capture variations in how people might represent words. For example, if you wanted to know how often people were talking about change in a system, you would want to capture whenever people say \"change\", \"changing\", \"changes\", or \"changed\". You can see how this would happen for stemming vs lemmatizing below.\n",
    "\n",
    "| Stemming | Lemmatizing |\n",
    "| --- | --- | \n",
    "| change $\\rightarrow$ chang | change $\\rightarrow$ change | \n",
    "| changes $\\rightarrow$ chang | changes $\\rightarrow$ change | \n",
    "| changing $\\rightarrow$ chang | changing $\\rightarrow$ change | \n",
    "| changed $\\rightarrow$ chang | changed $\\rightarrow$ change | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stem each word \n",
    "#initialze Stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "#apply to each word in each document\n",
    "bodytext_stemmed = bodytext.apply(lambda x: [stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8543578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view our sample text after being stemmed\n",
    "print(bodytext_stemmed.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12929c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize each word\n",
    "#initialize Lemmatizer\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "#apply to each word in each document\n",
    "bodytext_lemm = bodytext.apply(lambda x: [wnl.lemmatize(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view our sample text after being lemmatized\n",
    "print(bodytext_lemm.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b3f05",
   "metadata": {},
   "source": [
    "#### NLTK has other powerful accessories!\n",
    "\n",
    "nltk can help identify the part of speech to isolate nouns, verbs, adjectives, etc. It can also identify groupings of words that most often occur together!\n",
    "\n",
    "The nltk POS codes are: \n",
    "\n",
    "\n",
    "| Code | Part of Speech | Code | Part of Speech |\n",
    "| --- | --- | --- | --- |  \n",
    "|CC:| conjunction, coordinating |PDT:| pre-determiner |\n",
    "|CD:| numeral, cardinal |POS:| genitive marker |\n",
    "|DT:| determiner |PRP:| pronoun, personal |\n",
    "|EX:| existential there |RB:| adverb |\n",
    "|IN:| preposition or conjunction, subordinating |RP:| particle |\n",
    "|JJ:| adjective or numeral, ordinal |TO:| \"to\" as preposition or infinitive marker |\n",
    "|JJR:| adjective, comparative |UH:| interjection |\n",
    "|JJS:| adjective, superlative |VB:| verb, base form |\n",
    "|LS:| list item marker |VBD:| verb, past tense |\n",
    "|MD:| modal auxiliary |VBG:| verb, present participle or gerund |\n",
    "|NN:| noun, common, singular or mass |VBN:| verb, past participle |\n",
    "|NNP:| noun, proper, singular | WDT:| WH-determiner |\n",
    "|NNS:| noun, common, plural|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a50fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the part of speech and isolate adjectives, nouns, etc.\n",
    "example_sentence = bodytext.iloc[0]\n",
    "print(nltk.pos_tag(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at all of the adjectives for the postings\n",
    "def keep_pos(x,pos=['JJ','JJS','JJR']):\n",
    "    tagged = nltk.pos_tag(x)\n",
    "    words_to_keep = [t[0] for t in tagged if t[1] in pos]\n",
    "    return words_to_keep\n",
    "\n",
    "keep_pos(example_sentence, pos=['JJ','JJS','JJR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify words that often appear together\n",
    "number_of_words = 10\n",
    "ngrams = no_stopwords.apply(lambda x: list(nltk.ngrams(x,number_of_words)))\n",
    "count = Counter(list(chain.from_iterable(list(ngrams.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now your turn\n",
    "#Experiment with different regex commands to see how the text changes\n",
    "#Identify the most common words across the whole dataset at each stage to see how the list changes: \n",
    "#with the original data, with lowercasing, with removing stopwords, with stemming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4edc86",
   "metadata": {},
   "source": [
    "### 2. Introduction to TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c679bb",
   "metadata": {},
   "source": [
    "Step 2 of the NLP process is determining what your \"document\" will be. This can be the whole text as one, each sentence individually, or even bi- or tri-grams of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split by sentence\n",
    "def split_by_sent(text, split_criteria=['  ','.', '!', '?','\\n']):\n",
    "    for x in split_criteria:\n",
    "        text = str(text).replace(x, '*')\n",
    "    bodylist = str(text).split('*')\n",
    "    bodylist = [w for w in bodylist if w != '']\n",
    "    return bodylist    \n",
    "    \n",
    "sentences = data['description'].str.lower().apply(lambda x: split_by_sent(x))\n",
    "sentencedf = sentences.explode()\n",
    "sentencedf = sentencedf[~sentencedf.isna()]\n",
    "print(sentences.iloc[0])\n",
    "print('\\n', sentencedf.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71192ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split by bigram\n",
    "bigrams = no_stopwords.apply(lambda x: list(nltk.ngrams(x,2)))\n",
    "bigramdf = bigrams.explode()\n",
    "print(bigrams.iloc[0])\n",
    "print('\\n', bigramdf.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a438f02",
   "metadata": {},
   "source": [
    "##### TF-IDF Vectors\n",
    "One method of performing step 3, turning each document into a vector, is through Term Frequency-Inverse Document Frequency (TFIDF). TF-IDF measures how important each word is to each document. \n",
    "\n",
    "Term Frequency (tf) refers to how often a word occurs in a document, ranging from 0 to 1. Inverse document frequency (idf) refers to how often a word occurs in _any_ of the documents, where closer to 0 represents more common words (think: and, the, it) and closer to 1 represents rarer words (think: quire, ulotrichous).\n",
    "\n",
    "The goal is to have a vector for each document that is 1 x n (n being the total number of words in the dataset dictionary) with values describing the tf * idf scores for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we need a vector that shows the counts of each word in each document. Most of it will be 0.\n",
    "documents = bodytext.apply(lambda x: ' '.join(x))\n",
    "count_vect = CountVectorizer()\n",
    "data_counts = count_vect.fit_transform(documents)\n",
    "#Then, we can create the tf-idf matrix\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_tfidf = tfidf_transformer.fit_transform(data_counts)\n",
    "#Inspect the shape of the matrix\n",
    "print(data_counts.shape)\n",
    "print(data_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc77953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now with the sentence dataframe\n",
    "#First, we need a vector that shows the counts of each word in each document. Most of it will be 0.\n",
    "count_vect_sent = CountVectorizer()\n",
    "data_counts_sentences = count_vect_sent.fit_transform(sentencedf)\n",
    "#Then, we can create the tf-idf matrix\n",
    "tfidf_transformer_sent = TfidfTransformer()\n",
    "data_tfidf_sentences = tfidf_transformer_sent.fit_transform(data_counts_sentences)\n",
    "#Inspect the shape of the matrix\n",
    "print(data_counts_sentences.shape)\n",
    "print(data_tfidf_sentences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1e818",
   "metadata": {},
   "source": [
    "### 3. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10e296",
   "metadata": {},
   "source": [
    "Now that we have our documents represented as a matrix (m documents x n words in dictionary), we want to understand what topics are present "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eff652",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is an unsupervised topic modeling technique. We can use this technique to create clusters, or topics, that are commonly occuring across all of the documents. Then, we can understand what words describe those topics. Finally, we can trace the topics back to our documents (remember, this can be the full ad or a single sentence) and see what topics appear in each document. There can be more than one topic per document!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b734ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess our listings and break into sentences \n",
    "\n",
    "#lowercase\n",
    "bodytext = data['description'].str.lower()\n",
    "#remove stopwords\n",
    "bodytext = bodytext.apply(lambda x: str(x).split(\" \"))\n",
    "bodytext = bodytext.apply(lambda x: sorted(set(x) - set(nltk.corpus.stopwords.words('english')), key=x.index))\n",
    "#stem words\n",
    "bodytext = bodytext.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "#split into sentences\n",
    "sentences = bodytext.apply(lambda x: split_by_sent(\" \".join(x)))\n",
    "documents = sentences.explode()\n",
    "#remove empty sentences and extra spaces\n",
    "documents = documents[~documents.isna()]\n",
    "documents = documents.apply(lambda x: x.split(\" \"))\n",
    "documents = documents.apply(lambda x: [w for w in x if len(w)>0])\n",
    "documents = documents[documents.apply(lambda x: len(x)>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1161e5a-75ba-497f-a670-ea6d68982eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary\n",
    "all_text = list(documents)\n",
    "all_dict = corpora.Dictionary(all_text)\n",
    "#This is equivalent to the count vectorizer above\n",
    "doc_term_matrix = [all_dict.doc2bow(i) for i in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose number of topics and create model\n",
    "num_topics = 10\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrix,\n",
    "                             id2word=all_dict,\n",
    "                             num_topics=num_topics,\n",
    "                             eval_every=None,\n",
    "                             passes=1,\n",
    "                             random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the top num_words for each topic \n",
    "num_words = 15\n",
    "print_topics = ldamodel.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "\n",
    "\n",
    "for topic in print_topics:\n",
    "    print('Topic {}'.format(topic[0]))\n",
    "    topwords = topic[1].split('\"')[1::2]\n",
    "    print(\", \".join(topwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5648f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the most probable topic per sentence\n",
    "doc_top_topics = []\n",
    "for i in range(len(documents)):\n",
    "    topic_probs = ldamodel[doc_term_matrix[i]]\n",
    "    max_score = 0\n",
    "    top_topic = num_topics\n",
    "    for topic, prob in topic_probs:\n",
    "        if prob > max_score:\n",
    "            max_score = prob\n",
    "            top_topic = topic\n",
    "    doc_top_topics.append(top_topic)\n",
    "\n",
    "#create a dataframe\n",
    "sentencedf2 = pd.DataFrame({'adindex': documents.index, \n",
    "                            'sentence': documents.values, \n",
    "                            'top_topic': doc_top_topics, \n",
    "                            'sent_len': documents.apply(len)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b269f69-d7a4-4b41-93bf-eccfa61df674",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencedf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate what percentage of the ad is dedicated to each topic \n",
    "import numpy as np\n",
    "percentages = np.zeros((len(data),num_topics))\n",
    "#groupby the ad and the topic of the sentence. Sum the number of words per ad per topic\n",
    "groupeddf = sentencedf2.groupby(['adindex', 'top_topic']).sent_len.sum()\n",
    "#Put into a matrix\n",
    "for idx in groupeddf.index:\n",
    "    percentages[idx] = groupeddf[idx]\n",
    "percentages = np.transpose(np.transpose(percentages)/percentages.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b0c5c-799e-4294-9322-f61c27012ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the percentage of the ads dedicated to each topic \n",
    "pd.DataFrame(data=percentages, columns = range(num_topics)).boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now your turn\n",
    "#Look at the topics determined by our LDA method when using the whole ad \n",
    "#Or try to use our tf-idf vectors in another clustering method you know (k-means, dbscan, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff60b8-afe3-40da-80d6-e746ba86c3aa",
   "metadata": {},
   "source": [
    "### 4. Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6848a6-28b4-4fc4-adc1-c8d2c7171734",
   "metadata": {},
   "source": [
    "LDA represents an unsupervised NLP model. We can also use supervised models, or those with training labels, to understand more from text data. \n",
    "\n",
    "Many of the supervised methods you've learned, including SVMs, can be used in conjuction with vectors representing text data (such as those from TF-IDF). \n",
    "\n",
    "Another method is to use Large Language Models (LLMs). These are models that have already been trained on huge corpora of text sources. Initially, this training is unsupervised. The models iteratively learn relationships between words and phrases in order to place context and meaning into a semantic encoding space. Then, supervised techniques are used to fine-tune the models for specific tasks. You can either do this fine-tuning yourself or use further pretrained models from someone else for your task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6029d-0813-4008-90d5-f895b51b76d1",
   "metadata": {},
   "source": [
    "A great place to start for pretrained LLMs is https://huggingface.co/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39350655-4636-4410-9324-3192f5597f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##You may need to run this code outside of Jupyter Notebook \n",
    "\n",
    "#transformers is the modeling package from huggingface\n",
    "from transformers import pipeline\n",
    "#every model can be downloaded through its model path name\n",
    "model_pathname = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "#initialize the pipieline with the task (sentiment analysis) and the model (model name). Tokenizer is not always required\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_pathname, tokenizer=model_pathname)\n",
    "#pass a sentence on to the model \n",
    "sentiment_task(\"We can't wait for you to see this home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9b620-0939-45b8-8468-eb4ab2bb9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "##You may need to run this code outside of Jupyter Notebook \n",
    "\n",
    "#We can also perform \"zero shot\" classification where we also pass the expected labels \n",
    "text = \"discover marble hill, a neighborhood rich with history and populated with homes that are fully detached and have private yards & front porches\"\n",
    "#This is what we want the LLM to return. Similar to a GenAI prompt\n",
    "hypothesis_template = \"This text is about {}\"\n",
    "#These are our hypothesized topics\n",
    "classes_verbalized = [\"location\", \"amenities\", \"people\", \"logistics\"]\n",
    "#initialize the pipeline\n",
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\") \n",
    "#pass a sentence to the classifier\n",
    "output = zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=False)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640acf58",
   "metadata": {},
   "source": [
    "### 5. Putting it Together with Spatial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76782912",
   "metadata": {},
   "source": [
    "Once you have performed your text analysis, often you will end up with quantitative variables which can then be analyzed spatially as with any other data. \n",
    "\n",
    "You might have now integer values representing the most prominent topic for each document, the percent of the text dedicated to a word or topic, or even simply the boolean presence of a word or topic. If the documents contain some sort of spatial information (e.g., location of the Zillow ad), you can now perform your spatial analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640dbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.colormap as cm\n",
    "\n",
    "#add the topic percentages to the original dataframe\n",
    "data_new = data.join(pd.DataFrame(data=percentages, columns = range(num_topics)).fillna(0)) \n",
    "data_new = data_new[~data_new.latitude.isna()]\n",
    "\n",
    "#create the map\n",
    "centerlat = (data_new['latitude'].max() + data_new['latitude'].min()) / 2\n",
    "centerlong = (data_new['longitude'].max() + data_new['longitude'].min()) / 2\n",
    "center = (centerlat, centerlong)\n",
    "colormap = cm.LinearColormap(colors=['green', 'yellow', 'red'], vmin=0, vmax=1)\n",
    "map_nyc = folium.Map(location=center, zoom_start=10, tiles='OpenStreetMap')\n",
    "\n",
    "#topic_data1\n",
    "topic_number1 = 7\n",
    "for i in range(len(data_new)):\n",
    "    folium.Circle(\n",
    "        location=[data_new.iloc[i]['latitude'], data_new.iloc[i]['longitude']],\n",
    "        radius=10,\n",
    "        fill=True,\n",
    "        color=colormap(data_new.iloc[i][topic_number1]),\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(map_nyc)\n",
    "\n",
    "# the following line adds the scale directly to our map\n",
    "map_nyc.add_child(colormap)\n",
    "\n",
    "map_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc802a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_nyc2 = folium.Map(location=center, zoom_start=10, tiles='OpenStreetMap')\n",
    "\n",
    "#topic_data2\n",
    "topic_number2 = 8\n",
    "for i in range(len(data_new)):\n",
    "    folium.Circle(\n",
    "        location=[data_new.iloc[i]['latitude'], data_new.iloc[i]['longitude']],\n",
    "        radius=10,\n",
    "        fill=True,\n",
    "        color=colormap(data_new.iloc[i][topic_number2]),\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(map_nyc2)\n",
    "\n",
    "# the following line adds the scale directly to our map\n",
    "map_nyc2.add_child(colormap)\n",
    "\n",
    "map_nyc2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4af73",
   "metadata": {},
   "source": [
    "### 6. Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c5464",
   "metadata": {},
   "source": [
    "Work through the above examples to identify a pattern of your choosing. \n",
    "Separate the data initially and see how your topics vary. \n",
    "\n",
    "For example, what LDA topics emerge when you separate on listing price? on number of bedrooms? on square footage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d50150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c53dec24",
   "metadata": {},
   "source": [
    "### 7. BONUS - Working with Different Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect the language(s) of your text along with a confidence score\n",
    "from googletrans import Translator\n",
    "def detect_lang(text):\n",
    "    translator = Translator()\n",
    "    detection=translator.detect(text)\n",
    "    return  detection.confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_lang(bodytext.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
